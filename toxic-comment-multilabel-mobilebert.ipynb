{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-03T02:36:49.172034Z","iopub.execute_input":"2021-09-03T02:36:49.172357Z","iopub.status.idle":"2021-09-03T02:36:49.185416Z","shell.execute_reply.started":"2021-09-03T02:36:49.172289Z","shell.execute_reply":"2021-09-03T02:36:49.184439Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1. Setup","metadata":{}},{"cell_type":"code","source":"## Transformer\n!pip install transformers --quiet\n!pip install datasets transformers[sentencepiece] --quiet\n!pip install \"transformers[sentencepiece]\" --quiet","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:36:49.189221Z","iopub.execute_input":"2021-09-03T02:36:49.189514Z","iopub.status.idle":"2021-09-03T02:37:11.590669Z","shell.execute_reply.started":"2021-09-03T02:36:49.189489Z","shell.execute_reply":"2021-09-03T02:37:11.589701Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# import library\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport time\nimport datetime","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:37:11.594019Z","iopub.execute_input":"2021-09-03T02:37:11.594299Z","iopub.status.idle":"2021-09-03T02:37:12.548246Z","shell.execute_reply.started":"2021-09-03T02:37:11.594268Z","shell.execute_reply":"2021-09-03T02:37:12.547310Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_csv = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_csv_labels = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\nprint(df.columns)\nprint(df.shape)\ntarget_col= df.columns[2:]\nfeature_col= df.columns[1:2]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:37:12.550043Z","iopub.execute_input":"2021-09-03T02:37:12.550437Z","iopub.status.idle":"2021-09-03T02:37:16.796586Z","shell.execute_reply.started":"2021-09-03T02:37:12.550397Z","shell.execute_reply":"2021-09-03T02:37:16.795813Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate'],\n      dtype='object')\n(159571, 8)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = df.rename(columns={\"id\": \"idx\"})","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:37:16.798167Z","iopub.execute_input":"2021-09-03T02:37:16.798525Z","iopub.status.idle":"2021-09-03T02:37:16.812354Z","shell.execute_reply.started":"2021-09-03T02:37:16.798488Z","shell.execute_reply":"2021-09-03T02:37:16.811395Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"categories = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:37:55.473041Z","iopub.execute_input":"2021-09-03T02:37:55.473438Z","iopub.status.idle":"2021-09-03T02:37:55.478437Z","shell.execute_reply.started":"2021-09-03T02:37:55.473406Z","shell.execute_reply":"2021-09-03T02:37:55.477220Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# 1. Pre-processing & Modelling","metadata":{}},{"cell_type":"code","source":"# import pyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n# import other parts of transformers as well as tqdm\nfrom transformers import (AutoTokenizer, AutoModel, \n                          AutoModelForSequenceClassification, \n                          DataCollatorWithPadding, AdamW, get_scheduler,\n                          get_linear_schedule_with_warmup,\n                          )\n# import pyarrow (can only import this with GPU on kaggle notebook)\nimport pyarrow as pa\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport datasets\nimport random\nfrom sklearn.metrics import classification_report, hamming_loss, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:37:16.813592Z","iopub.execute_input":"2021-09-03T02:37:16.813937Z","iopub.status.idle":"2021-09-03T02:37:23.328065Z","shell.execute_reply.started":"2021-09-03T02:37:16.813901Z","shell.execute_reply":"2021-09-03T02:37:23.327254Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Setting up seed value\nseed_value = 42\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:37:23.329286Z","iopub.execute_input":"2021-09-03T02:37:23.329612Z","iopub.status.idle":"2021-09-03T02:37:23.340774Z","shell.execute_reply.started":"2021-09-03T02:37:23.329577Z","shell.execute_reply":"2021-09-03T02:37:23.339824Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Train, Validation and Test Split","metadata":{}},{"cell_type":"code","source":"# train & validation, test split\ntrain_val_df, test_df = train_test_split(df[[\"idx\", \"comment_text\"] + categories], test_size=0.2, random_state=seed_value)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:38:04.034171Z","iopub.execute_input":"2021-09-03T02:38:04.034520Z","iopub.status.idle":"2021-09-03T02:38:04.087323Z","shell.execute_reply.started":"2021-09-03T02:38:04.034489Z","shell.execute_reply":"2021-09-03T02:38:04.086456Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# train, validation split\ntrain_df, val_df, = train_test_split(train_val_df[[\"idx\", \"comment_text\"] + categories], test_size=0.25, random_state=seed_value)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:38:04.088769Z","iopub.execute_input":"2021-09-03T02:38:04.089136Z","iopub.status.idle":"2021-09-03T02:38:04.127290Z","shell.execute_reply.started":"2021-09-03T02:38:04.089094Z","shell.execute_reply":"2021-09-03T02:38:04.126448Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(f\"Size of train, validation and test are {len(train_df)}, {len(val_df)}, {len(test_df)} respectively.\")\nprint(f\"Proportion of train, validation and test are {round(len(train_df)/len(df),2)}, {round(len(val_df)/len(df),2)}, {round(len(test_df)/len(df),2)} respectively.\")","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:38:04.129187Z","iopub.execute_input":"2021-09-03T02:38:04.129544Z","iopub.status.idle":"2021-09-03T02:38:04.135138Z","shell.execute_reply.started":"2021-09-03T02:38:04.129508Z","shell.execute_reply":"2021-09-03T02:38:04.134111Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Size of train, validation and test are 95742, 31914, 31915 respectively.\nProportion of train, validation and test are 0.6, 0.2, 0.2 respectively.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.reset_index(inplace=True)\ntrain_df.drop(\"index\", axis=1, inplace=True)\n\nval_df.reset_index(inplace=True)\nval_df.drop(\"index\", axis=1, inplace=True)\n\ntest_df.reset_index(inplace=True)\ntest_df.drop(\"index\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:38:04.136761Z","iopub.execute_input":"2021-09-03T02:38:04.137280Z","iopub.status.idle":"2021-09-03T02:38:04.166050Z","shell.execute_reply.started":"2021-09-03T02:38:04.137243Z","shell.execute_reply":"2021-09-03T02:38:04.165242Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:38:04.167262Z","iopub.execute_input":"2021-09-03T02:38:04.167782Z","iopub.status.idle":"2021-09-03T02:38:04.182433Z","shell.execute_reply.started":"2021-09-03T02:38:04.167744Z","shell.execute_reply":"2021-09-03T02:38:04.181442Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                idx                                       comment_text  toxic  \\\n0  f8514b48bbea9f52  So when the polls close in Ontario at 9:30 pm,...      0   \n1  4c5e853732b4aa2d                          from blowing up the Earth      0   \n2  097dc7f0c8a482a8  =fRENCH lANGUAGE\\nThis is the National Library...      0   \n3  644c631403602fb9  Wiley Protocol \\n\\nKnow anything about bioiden...      0   \n4  ca98b70deab8bcfc  \"\\n.\\nNo, the facts are pretty clear. You chan...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f8514b48bbea9f52</td>\n      <td>So when the polls close in Ontario at 9:30 pm,...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4c5e853732b4aa2d</td>\n      <td>from blowing up the Earth</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>097dc7f0c8a482a8</td>\n      <td>=fRENCH lANGUAGE\\nThis is the National Library...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>644c631403602fb9</td>\n      <td>Wiley Protocol \\n\\nKnow anything about bioiden...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ca98b70deab8bcfc</td>\n      <td>\"\\n.\\nNo, the facts are pretty clear. You chan...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenizing Text in Batches","metadata":{}},{"cell_type":"code","source":"# instantiate tokenizer, model using \"google/mobilebert-uncased\"\ncheckpoint = \"google/mobilebert-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:38:04.184085Z","iopub.execute_input":"2021-09-03T02:38:04.184796Z","iopub.status.idle":"2021-09-03T02:38:09.120840Z","shell.execute_reply.started":"2021-09-03T02:38:04.184744Z","shell.execute_reply":"2021-09-03T02:38:09.120031Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/847 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f74db9a1f0094a60a2bee37676db2c98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f05bf3fd43a44abd8654c92565337317"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"812b6fd086f34fb491ad062898fd7c95"}},"metadata":{}}]},{"cell_type":"code","source":"# tokenize and encode sequences in the training set\ntrain_tokens = tokenizer.batch_encode_plus(train_df[\"comment_text\"].tolist(),\n                                           max_length = 200,\n                                           pad_to_max_length=True,\n                                           truncation=True,\n                                           return_token_type_ids=False\n                                           )\n\n# tokenize and encode sequences in the validation set\nval_tokens = tokenizer.batch_encode_plus(val_df[\"comment_text\"].tolist(),\n                                         max_length = 200,\n                                         pad_to_max_length=True,\n                                         truncation=True,\n                                         return_token_type_ids=False\n                                         )\n\n# tokenize and encode sequences in the test set\ntest_tokens = tokenizer.batch_encode_plus(test_df[\"comment_text\"].tolist(),\n                                          max_length = 200,\n                                          pad_to_max_length=True,\n                                          truncation=True,\n                                          return_token_type_ids=False\n                                          )","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:38:09.122064Z","iopub.execute_input":"2021-09-03T02:38:09.122417Z","iopub.status.idle":"2021-09-03T02:38:54.644428Z","shell.execute_reply.started":"2021-09-03T02:38:09.122380Z","shell.execute_reply":"2021-09-03T02:38:54.643552Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"## convert lists to tensors\ntrain_seq = torch.tensor(train_tokens['input_ids'])\ntrain_mask = torch.tensor(train_tokens['attention_mask'])\n# change from a list of 1 & 0 to array then to tensor\ntrain_y = torch.tensor(np.array(train_df[categories]))  \n\nval_seq = torch.tensor(val_tokens['input_ids'])\nval_mask = torch.tensor(val_tokens['attention_mask'])\n# change from a list of 1 & 0 to array then to tensor\nval_y = torch.tensor(np.array(val_df[categories]))\n\ntest_seq = torch.tensor(test_tokens['input_ids'])\ntest_mask = torch.tensor(test_tokens['attention_mask'])\n# change from a list of 1 & 0 to array then to tensor\ntest_y = torch.tensor(np.array(test_df[categories]))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:38:54.647086Z","iopub.execute_input":"2021-09-03T02:38:54.647475Z","iopub.status.idle":"2021-09-03T02:39:01.575946Z","shell.execute_reply.started":"2021-09-03T02:38:54.647436Z","shell.execute_reply":"2021-09-03T02:39:01.575073Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_y","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:01.577863Z","iopub.execute_input":"2021-09-03T02:39:01.578251Z","iopub.status.idle":"2021-09-03T02:39:01.599997Z","shell.execute_reply.started":"2021-09-03T02:39:01.578213Z","shell.execute_reply":"2021-09-03T02:39:01.598964Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([[0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        ...,\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0]])"},"metadata":{}}]},{"cell_type":"code","source":"# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# wrap tensors\ntest_data = TensorDataset(test_seq, test_mask, test_y)\ntest_sampler = SequentialSampler(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:01.601363Z","iopub.execute_input":"2021-09-03T02:39:01.601729Z","iopub.status.idle":"2021-09-03T02:39:01.607251Z","shell.execute_reply.started":"2021-09-03T02:39:01.601691Z","shell.execute_reply":"2021-09-03T02:39:01.606216Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# define a batch size\nbatch_size = 32\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, \n                              sampler=train_sampler, \n                              batch_size=batch_size,\n                              # collate_fn=data_collator\n                              )\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, \n                            sampler = val_sampler, \n                            batch_size=batch_size,\n                            # collate_fn=data_collator\n                            )","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:01.608924Z","iopub.execute_input":"2021-09-03T02:39:01.609541Z","iopub.status.idle":"2021-09-03T02:39:01.616091Z","shell.execute_reply.started":"2021-09-03T02:39:01.609505Z","shell.execute_reply":"2021-09-03T02:39:01.615306Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"No dynamic padding. Above accomodate for fix padding size of 200","metadata":{}},{"cell_type":"code","source":"for step,batch in enumerate(train_dataloader):\n    break\nprint(batch[0])\nprint(batch[1])\nprint(batch[2])","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:01.617430Z","iopub.execute_input":"2021-09-03T02:39:01.617937Z","iopub.status.idle":"2021-09-03T02:39:01.659614Z","shell.execute_reply.started":"2021-09-03T02:39:01.617898Z","shell.execute_reply":"2021-09-03T02:39:01.658850Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"tensor([[  101, 19169, 16407,  ...,     0,     0,     0],\n        [  101, 27105, 15693,  ...,     0,     0,     0],\n        [  101,  2023,  2003,  ...,     0,     0,     0],\n        ...,\n        [  101,  3398,  2092,  ...,     0,     0,     0],\n        [  101,  2025,  2043,  ...,     0,     0,     0],\n        [  101,  2021,  1045,  ...,     0,     0,     0]])\ntensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])\ntensor([[0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [1, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setting up MobileBert Model (Even lighter version of DistilBert)","metadata":{}},{"cell_type":"code","source":"# instantiate model using MobileBert\"\n# Will be training for the whole network instead of just finetuning the weight of the head layer\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = 6)\n\n# In the event that we only need to finetune the head layer and not the whole network,\n# just need to uncomment the below code\n\n# for param in model.bert.parameters():\n#     param.requires_grad = False\n\n# for name, param in model.named_parameters():                \n#     if param.requires_grad:\n#         print(name)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:01.660665Z","iopub.execute_input":"2021-09-03T02:39:01.660988Z","iopub.status.idle":"2021-09-03T02:39:08.957567Z","shell.execute_reply.started":"2021-09-03T02:39:01.660962Z","shell.execute_reply":"2021-09-03T02:39:08.956729Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/147M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88cd4478ddd94c12bdd4bd914e29a2aa"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:08.961212Z","iopub.execute_input":"2021-09-03T02:39:08.963186Z","iopub.status.idle":"2021-09-03T02:39:14.187835Z","shell.execute_reply.started":"2021-09-03T02:39:08.963126Z","shell.execute_reply":"2021-09-03T02:39:14.186957Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"MobileBertForSequenceClassification(\n  (mobilebert): MobileBertModel(\n    (embeddings): MobileBertEmbeddings(\n      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 512)\n      (token_type_embeddings): Embedding(2, 512)\n      (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n      (LayerNorm): NoNorm()\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): MobileBertEncoder(\n      (layer): ModuleList(\n        (0): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (1): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (2): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (3): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (4): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (5): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (6): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (7): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (8): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (9): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (10): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (11): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (12): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (13): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (14): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (15): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (16): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (17): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (18): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (19): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (20): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (21): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (22): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (23): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n      )\n    )\n    (pooler): MobileBertPooler()\n  )\n  (dropout): Dropout(p=0.0, inplace=False)\n  (classifier): Linear(in_features=512, out_features=6, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"Also need to set up the following before training:\n\n1. Optimizer\n2. Scheduler (if want to go add a little decay in the learning rate)\n3. Get the right loss function/criterion\n4. Specify the right measurement for accuracy (since this is a multi-label problem)","metadata":{}},{"cell_type":"code","source":"# Set up optimizer\n# Will be training for the whole network instead of just finetuning the weight of the head layer\nLEARN_RATE = 3e-5\noptimizer = AdamW(model.parameters(),\n                  lr = LEARN_RATE, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                  )\n\n# In the event that we only need to finetune the head layer and not the whole network,\n# just need to uncomment the below code\n\n# optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n#                   lr = LEARN_RATE, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n#                   eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n#                   )\n\n# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = 1e-5)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:14.189077Z","iopub.execute_input":"2021-09-03T02:39:14.189572Z","iopub.status.idle":"2021-09-03T02:39:14.212086Z","shell.execute_reply.started":"2021-09-03T02:39:14.189533Z","shell.execute_reply":"2021-09-03T02:39:14.211242Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n# We chose to run for 3\nepochs = 3\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # default value\n                                            num_training_steps = total_steps\n                                            )\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:14.213257Z","iopub.execute_input":"2021-09-03T02:39:14.213610Z","iopub.status.idle":"2021-09-03T02:39:14.220915Z","shell.execute_reply.started":"2021-09-03T02:39:14.213571Z","shell.execute_reply":"2021-09-03T02:39:14.220204Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Specify loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n# This criterion combines LogSoftmax and NLLLoss in one single class.\n# More relevant for multiclass (because of the softmax)\n\n# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n# More relevant for multilabel. For more explanation, can refer to the link below.\n# https://discuss.pytorch.org/t/using-bcewithlogisloss-for-multi-label-classification/67011/2","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:14.221844Z","iopub.execute_input":"2021-09-03T02:39:14.222083Z","iopub.status.idle":"2021-09-03T02:39:14.235507Z","shell.execute_reply.started":"2021-09-03T02:39:14.222059Z","shell.execute_reply":"2021-09-03T02:39:14.234618Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Specify measurement of accuracy\ndef accuracy_thresh(y_pred, y_true, thresh:float=0.4, sigmoid:bool=True):\n    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n    if sigmoid: \n        y_pred = y_pred.sigmoid()\n    # return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n    return np.mean(((y_pred>thresh).float()==y_true.float()).float().cpu().numpy(), axis=1).sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:14.238311Z","iopub.execute_input":"2021-09-03T02:39:14.238588Z","iopub.status.idle":"2021-09-03T02:39:14.245591Z","shell.execute_reply.started":"2021-09-03T02:39:14.238543Z","shell.execute_reply":"2021-09-03T02:39:14.244726Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:14.247932Z","iopub.execute_input":"2021-09-03T02:39:14.248405Z","iopub.status.idle":"2021-09-03T02:39:14.254029Z","shell.execute_reply.started":"2021-09-03T02:39:14.248370Z","shell.execute_reply":"2021-09-03T02:39:14.253115Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    # ========================================\n    #               Training\n    # ========================================\n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n    # Reset the total loss & accuracy for this epoch.\n    total_train_loss = 0\n    total_train_accuracy = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a backward pass. \n        # PyTorch doesn't do this automatically because accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # The documentation for this `model` function is here: \n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n        # It returns different numbers of parameters depending on what arguments are given and what flags are set. \n        # For our useage here, it returns the loss (because we provided labels) and the \"logits\"--the model outputs prior to activation.\n        output = model(b_input_ids, \n                       # token_type_ids=None, \n                       attention_mask=b_input_mask, \n                       #labels=b_labels\n                       )\n        logits = output.logits\n        loss = criterion(logits, b_labels.float())\n\n        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. \n        # `loss` is a Tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\n        total_train_loss += loss.item()\n        \n        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n        total_train_accuracy += accuracy_thresh(logits,b_labels.float())\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Report the final accuracy for this validation run.\n    # avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n    # print(\"  Accuracy: {0:.5f}\".format(avg_train_accuracy))\n    train_accuracy = total_train_accuracy / len(train_df)\n    print(\"  Accuracy: {0:.5f}\".format(train_accuracy))\n    \n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n    model.eval()\n\n    # Tracking variables \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in val_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the \"segment ids\", which differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            # Get the \"logits\" output by the model. The \"logits\" are the output values prior to applying an activation function like the softmax.\n            output = model(b_input_ids, \n                           # token_type_ids=None, \n                           attention_mask=b_input_mask, \n                           # labels=b_labels\n                           )\n            \n            logits = output.logits\n            loss = criterion(logits, b_labels.float())\n            \n        # Accumulate the validation loss.\n        total_eval_loss += loss.item()\n        \n        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n        total_eval_accuracy += accuracy_thresh(logits,b_labels.float())\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy() # maybe irrelevant over here\n        label_ids = b_labels.to('cpu').numpy() # maybe irrelevant over here\n\n    # Report the final accuracy for this validation run.\n    # avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n    # print(\"  Accuracy: {0:.5f}\".format(avg_val_accuracy))\n    val_accuracy = total_eval_accuracy / len(val_df)\n    print(\"  Accuracy: {0:.5f}\".format(val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(val_dataloader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            # 'Training Accur': avg_train_accuracy,\n            # 'Valid. Accur.': avg_val_accuracy,\n            'Training Accur': train_accuracy,\n            'Valid. Accur.': val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T02:39:14.255422Z","iopub.execute_input":"2021-09-03T02:39:14.255807Z","iopub.status.idle":"2021-09-03T04:00:46.176667Z","shell.execute_reply.started":"2021-09-03T02:39:14.255771Z","shell.execute_reply":"2021-09-03T04:00:46.175822Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 3 ========\nTraining...\n  Batch    40  of  2,992.    Elapsed: 0:00:22.\n  Batch    80  of  2,992.    Elapsed: 0:00:43.\n  Batch   120  of  2,992.    Elapsed: 0:01:05.\n  Batch   160  of  2,992.    Elapsed: 0:01:26.\n  Batch   200  of  2,992.    Elapsed: 0:01:47.\n  Batch   240  of  2,992.    Elapsed: 0:02:08.\n  Batch   280  of  2,992.    Elapsed: 0:02:29.\n  Batch   320  of  2,992.    Elapsed: 0:02:51.\n  Batch   360  of  2,992.    Elapsed: 0:03:11.\n  Batch   400  of  2,992.    Elapsed: 0:03:32.\n  Batch   440  of  2,992.    Elapsed: 0:03:53.\n  Batch   480  of  2,992.    Elapsed: 0:04:14.\n  Batch   520  of  2,992.    Elapsed: 0:04:35.\n  Batch   560  of  2,992.    Elapsed: 0:04:56.\n  Batch   600  of  2,992.    Elapsed: 0:05:16.\n  Batch   640  of  2,992.    Elapsed: 0:05:37.\n  Batch   680  of  2,992.    Elapsed: 0:05:58.\n  Batch   720  of  2,992.    Elapsed: 0:06:19.\n  Batch   760  of  2,992.    Elapsed: 0:06:39.\n  Batch   800  of  2,992.    Elapsed: 0:07:00.\n  Batch   840  of  2,992.    Elapsed: 0:07:21.\n  Batch   880  of  2,992.    Elapsed: 0:07:41.\n  Batch   920  of  2,992.    Elapsed: 0:08:02.\n  Batch   960  of  2,992.    Elapsed: 0:08:22.\n  Batch 1,000  of  2,992.    Elapsed: 0:08:43.\n  Batch 1,040  of  2,992.    Elapsed: 0:09:03.\n  Batch 1,080  of  2,992.    Elapsed: 0:09:24.\n  Batch 1,120  of  2,992.    Elapsed: 0:09:44.\n  Batch 1,160  of  2,992.    Elapsed: 0:10:05.\n  Batch 1,200  of  2,992.    Elapsed: 0:10:25.\n  Batch 1,240  of  2,992.    Elapsed: 0:10:46.\n  Batch 1,280  of  2,992.    Elapsed: 0:11:06.\n  Batch 1,320  of  2,992.    Elapsed: 0:11:27.\n  Batch 1,360  of  2,992.    Elapsed: 0:11:47.\n  Batch 1,400  of  2,992.    Elapsed: 0:12:08.\n  Batch 1,440  of  2,992.    Elapsed: 0:12:28.\n  Batch 1,480  of  2,992.    Elapsed: 0:12:49.\n  Batch 1,520  of  2,992.    Elapsed: 0:13:10.\n  Batch 1,560  of  2,992.    Elapsed: 0:13:30.\n  Batch 1,600  of  2,992.    Elapsed: 0:13:51.\n  Batch 1,640  of  2,992.    Elapsed: 0:14:11.\n  Batch 1,680  of  2,992.    Elapsed: 0:14:32.\n  Batch 1,720  of  2,992.    Elapsed: 0:14:53.\n  Batch 1,760  of  2,992.    Elapsed: 0:15:13.\n  Batch 1,800  of  2,992.    Elapsed: 0:15:33.\n  Batch 1,840  of  2,992.    Elapsed: 0:15:54.\n  Batch 1,880  of  2,992.    Elapsed: 0:16:14.\n  Batch 1,920  of  2,992.    Elapsed: 0:16:35.\n  Batch 1,960  of  2,992.    Elapsed: 0:16:55.\n  Batch 2,000  of  2,992.    Elapsed: 0:17:17.\n  Batch 2,040  of  2,992.    Elapsed: 0:17:37.\n  Batch 2,080  of  2,992.    Elapsed: 0:17:57.\n  Batch 2,120  of  2,992.    Elapsed: 0:18:18.\n  Batch 2,160  of  2,992.    Elapsed: 0:18:38.\n  Batch 2,200  of  2,992.    Elapsed: 0:18:59.\n  Batch 2,240  of  2,992.    Elapsed: 0:19:19.\n  Batch 2,280  of  2,992.    Elapsed: 0:19:40.\n  Batch 2,320  of  2,992.    Elapsed: 0:20:00.\n  Batch 2,360  of  2,992.    Elapsed: 0:20:21.\n  Batch 2,400  of  2,992.    Elapsed: 0:20:41.\n  Batch 2,440  of  2,992.    Elapsed: 0:21:02.\n  Batch 2,480  of  2,992.    Elapsed: 0:21:22.\n  Batch 2,520  of  2,992.    Elapsed: 0:21:43.\n  Batch 2,560  of  2,992.    Elapsed: 0:22:03.\n  Batch 2,600  of  2,992.    Elapsed: 0:22:24.\n  Batch 2,640  of  2,992.    Elapsed: 0:22:44.\n  Batch 2,680  of  2,992.    Elapsed: 0:23:05.\n  Batch 2,720  of  2,992.    Elapsed: 0:23:25.\n  Batch 2,760  of  2,992.    Elapsed: 0:23:46.\n  Batch 2,800  of  2,992.    Elapsed: 0:24:06.\n  Batch 2,840  of  2,992.    Elapsed: 0:24:27.\n  Batch 2,880  of  2,992.    Elapsed: 0:24:47.\n  Batch 2,920  of  2,992.    Elapsed: 0:25:08.\n  Batch 2,960  of  2,992.    Elapsed: 0:25:29.\n  Accuracy: 0.97389\n\n  Average training loss: 11136.71\n  Training epcoh took: 0:25:45\n\nRunning Validation...\n  Accuracy: 0.98186\n  Validation Loss: 0.04\n  Validation took: 0:01:37\n\n======== Epoch 2 / 3 ========\nTraining...\n  Batch    40  of  2,992.    Elapsed: 0:00:21.\n  Batch    80  of  2,992.    Elapsed: 0:00:41.\n  Batch   120  of  2,992.    Elapsed: 0:01:01.\n  Batch   160  of  2,992.    Elapsed: 0:01:22.\n  Batch   200  of  2,992.    Elapsed: 0:01:42.\n  Batch   240  of  2,992.    Elapsed: 0:02:03.\n  Batch   280  of  2,992.    Elapsed: 0:02:23.\n  Batch   320  of  2,992.    Elapsed: 0:02:44.\n  Batch   360  of  2,992.    Elapsed: 0:03:04.\n  Batch   400  of  2,992.    Elapsed: 0:03:25.\n  Batch   440  of  2,992.    Elapsed: 0:03:45.\n  Batch   480  of  2,992.    Elapsed: 0:04:05.\n  Batch   520  of  2,992.    Elapsed: 0:04:26.\n  Batch   560  of  2,992.    Elapsed: 0:04:46.\n  Batch   600  of  2,992.    Elapsed: 0:05:07.\n  Batch   640  of  2,992.    Elapsed: 0:05:27.\n  Batch   680  of  2,992.    Elapsed: 0:05:48.\n  Batch   720  of  2,992.    Elapsed: 0:06:08.\n  Batch   760  of  2,992.    Elapsed: 0:06:28.\n  Batch   800  of  2,992.    Elapsed: 0:06:49.\n  Batch   840  of  2,992.    Elapsed: 0:07:09.\n  Batch   880  of  2,992.    Elapsed: 0:07:30.\n  Batch   920  of  2,992.    Elapsed: 0:07:50.\n  Batch   960  of  2,992.    Elapsed: 0:08:10.\n  Batch 1,000  of  2,992.    Elapsed: 0:08:31.\n  Batch 1,040  of  2,992.    Elapsed: 0:08:51.\n  Batch 1,080  of  2,992.    Elapsed: 0:09:12.\n  Batch 1,120  of  2,992.    Elapsed: 0:09:32.\n  Batch 1,160  of  2,992.    Elapsed: 0:09:52.\n  Batch 1,200  of  2,992.    Elapsed: 0:10:13.\n  Batch 1,240  of  2,992.    Elapsed: 0:10:33.\n  Batch 1,280  of  2,992.    Elapsed: 0:10:53.\n  Batch 1,320  of  2,992.    Elapsed: 0:11:14.\n  Batch 1,360  of  2,992.    Elapsed: 0:11:34.\n  Batch 1,400  of  2,992.    Elapsed: 0:11:55.\n  Batch 1,440  of  2,992.    Elapsed: 0:12:15.\n  Batch 1,480  of  2,992.    Elapsed: 0:12:36.\n  Batch 1,520  of  2,992.    Elapsed: 0:12:56.\n  Batch 1,560  of  2,992.    Elapsed: 0:13:17.\n  Batch 1,600  of  2,992.    Elapsed: 0:13:37.\n  Batch 1,640  of  2,992.    Elapsed: 0:13:58.\n  Batch 1,680  of  2,992.    Elapsed: 0:14:18.\n  Batch 1,720  of  2,992.    Elapsed: 0:14:39.\n  Batch 1,760  of  2,992.    Elapsed: 0:14:59.\n  Batch 1,800  of  2,992.    Elapsed: 0:15:20.\n  Batch 1,840  of  2,992.    Elapsed: 0:15:40.\n  Batch 1,880  of  2,992.    Elapsed: 0:16:01.\n  Batch 1,920  of  2,992.    Elapsed: 0:16:21.\n  Batch 1,960  of  2,992.    Elapsed: 0:16:41.\n  Batch 2,000  of  2,992.    Elapsed: 0:17:02.\n  Batch 2,040  of  2,992.    Elapsed: 0:17:22.\n  Batch 2,080  of  2,992.    Elapsed: 0:17:43.\n  Batch 2,120  of  2,992.    Elapsed: 0:18:03.\n  Batch 2,160  of  2,992.    Elapsed: 0:18:24.\n  Batch 2,200  of  2,992.    Elapsed: 0:18:44.\n  Batch 2,240  of  2,992.    Elapsed: 0:19:04.\n  Batch 2,280  of  2,992.    Elapsed: 0:19:25.\n  Batch 2,320  of  2,992.    Elapsed: 0:19:45.\n  Batch 2,360  of  2,992.    Elapsed: 0:20:05.\n  Batch 2,400  of  2,992.    Elapsed: 0:20:26.\n  Batch 2,440  of  2,992.    Elapsed: 0:20:47.\n  Batch 2,480  of  2,992.    Elapsed: 0:21:07.\n  Batch 2,520  of  2,992.    Elapsed: 0:21:28.\n  Batch 2,560  of  2,992.    Elapsed: 0:21:48.\n  Batch 2,600  of  2,992.    Elapsed: 0:22:08.\n  Batch 2,640  of  2,992.    Elapsed: 0:22:29.\n  Batch 2,680  of  2,992.    Elapsed: 0:22:49.\n  Batch 2,720  of  2,992.    Elapsed: 0:23:10.\n  Batch 2,760  of  2,992.    Elapsed: 0:23:30.\n  Batch 2,800  of  2,992.    Elapsed: 0:23:51.\n  Batch 2,840  of  2,992.    Elapsed: 0:24:11.\n  Batch 2,880  of  2,992.    Elapsed: 0:24:32.\n  Batch 2,920  of  2,992.    Elapsed: 0:24:52.\n  Batch 2,960  of  2,992.    Elapsed: 0:25:12.\n  Accuracy: 0.98584\n\n  Average training loss: 0.04\n  Training epcoh took: 0:25:29\n\nRunning Validation...\n  Accuracy: 0.98467\n  Validation Loss: 0.04\n  Validation took: 0:01:37\n\n======== Epoch 3 / 3 ========\nTraining...\n  Batch    40  of  2,992.    Elapsed: 0:00:20.\n  Batch    80  of  2,992.    Elapsed: 0:00:41.\n  Batch   120  of  2,992.    Elapsed: 0:01:01.\n  Batch   160  of  2,992.    Elapsed: 0:01:22.\n  Batch   200  of  2,992.    Elapsed: 0:01:42.\n  Batch   240  of  2,992.    Elapsed: 0:02:02.\n  Batch   280  of  2,992.    Elapsed: 0:02:22.\n  Batch   320  of  2,992.    Elapsed: 0:02:43.\n  Batch   360  of  2,992.    Elapsed: 0:03:03.\n  Batch   400  of  2,992.    Elapsed: 0:03:23.\n  Batch   440  of  2,992.    Elapsed: 0:03:44.\n  Batch   480  of  2,992.    Elapsed: 0:04:05.\n  Batch   520  of  2,992.    Elapsed: 0:04:25.\n  Batch   560  of  2,992.    Elapsed: 0:04:45.\n  Batch   600  of  2,992.    Elapsed: 0:05:06.\n  Batch   640  of  2,992.    Elapsed: 0:05:26.\n  Batch   680  of  2,992.    Elapsed: 0:05:46.\n  Batch   720  of  2,992.    Elapsed: 0:06:07.\n  Batch   760  of  2,992.    Elapsed: 0:06:27.\n  Batch   800  of  2,992.    Elapsed: 0:06:48.\n  Batch   840  of  2,992.    Elapsed: 0:07:08.\n  Batch   880  of  2,992.    Elapsed: 0:07:29.\n  Batch   920  of  2,992.    Elapsed: 0:07:49.\n  Batch   960  of  2,992.    Elapsed: 0:08:09.\n  Batch 1,000  of  2,992.    Elapsed: 0:08:30.\n  Batch 1,040  of  2,992.    Elapsed: 0:08:50.\n  Batch 1,080  of  2,992.    Elapsed: 0:09:10.\n  Batch 1,120  of  2,992.    Elapsed: 0:09:31.\n  Batch 1,160  of  2,992.    Elapsed: 0:09:51.\n  Batch 1,200  of  2,992.    Elapsed: 0:10:12.\n  Batch 1,240  of  2,992.    Elapsed: 0:10:32.\n  Batch 1,280  of  2,992.    Elapsed: 0:10:52.\n  Batch 1,320  of  2,992.    Elapsed: 0:11:13.\n  Batch 1,360  of  2,992.    Elapsed: 0:11:33.\n  Batch 1,400  of  2,992.    Elapsed: 0:11:54.\n  Batch 1,440  of  2,992.    Elapsed: 0:12:14.\n  Batch 1,480  of  2,992.    Elapsed: 0:12:35.\n  Batch 1,520  of  2,992.    Elapsed: 0:12:55.\n  Batch 1,560  of  2,992.    Elapsed: 0:13:16.\n  Batch 1,600  of  2,992.    Elapsed: 0:13:36.\n  Batch 1,640  of  2,992.    Elapsed: 0:13:56.\n  Batch 1,680  of  2,992.    Elapsed: 0:14:17.\n  Batch 1,720  of  2,992.    Elapsed: 0:14:37.\n  Batch 1,760  of  2,992.    Elapsed: 0:14:57.\n  Batch 1,800  of  2,992.    Elapsed: 0:15:18.\n  Batch 1,840  of  2,992.    Elapsed: 0:15:38.\n  Batch 1,880  of  2,992.    Elapsed: 0:15:59.\n  Batch 1,920  of  2,992.    Elapsed: 0:16:19.\n  Batch 1,960  of  2,992.    Elapsed: 0:16:40.\n  Batch 2,000  of  2,992.    Elapsed: 0:17:00.\n  Batch 2,040  of  2,992.    Elapsed: 0:17:21.\n  Batch 2,080  of  2,992.    Elapsed: 0:17:41.\n  Batch 2,120  of  2,992.    Elapsed: 0:18:01.\n  Batch 2,160  of  2,992.    Elapsed: 0:18:22.\n  Batch 2,200  of  2,992.    Elapsed: 0:18:42.\n  Batch 2,240  of  2,992.    Elapsed: 0:19:03.\n  Batch 2,280  of  2,992.    Elapsed: 0:19:23.\n  Batch 2,320  of  2,992.    Elapsed: 0:19:43.\n  Batch 2,360  of  2,992.    Elapsed: 0:20:04.\n  Batch 2,400  of  2,992.    Elapsed: 0:20:24.\n  Batch 2,440  of  2,992.    Elapsed: 0:20:44.\n  Batch 2,480  of  2,992.    Elapsed: 0:21:05.\n  Batch 2,520  of  2,992.    Elapsed: 0:21:25.\n  Batch 2,560  of  2,992.    Elapsed: 0:21:46.\n  Batch 2,600  of  2,992.    Elapsed: 0:22:06.\n  Batch 2,640  of  2,992.    Elapsed: 0:22:27.\n  Batch 2,680  of  2,992.    Elapsed: 0:22:47.\n  Batch 2,720  of  2,992.    Elapsed: 0:23:08.\n  Batch 2,760  of  2,992.    Elapsed: 0:23:28.\n  Batch 2,800  of  2,992.    Elapsed: 0:23:49.\n  Batch 2,840  of  2,992.    Elapsed: 0:24:10.\n  Batch 2,880  of  2,992.    Elapsed: 0:24:30.\n  Batch 2,920  of  2,992.    Elapsed: 0:24:51.\n  Batch 2,960  of  2,992.    Elapsed: 0:25:11.\n  Accuracy: 0.98791\n\n  Average training loss: 0.03\n  Training epcoh took: 0:25:27\n\nRunning Validation...\n  Accuracy: 0.98388\n  Validation Loss: 0.04\n  Validation took: 0:01:37\n\nTraining complete!\nTotal training took 1:21:32 (h:mm:ss)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display floats with two decimal places.\npd.set_option('precision', 5)\n# Create a DataFrame from our training statistics.\ndf_stats = pd.DataFrame(data=training_stats)\n# Use the 'epoch' as the row index.\ndf_stats = df_stats.set_index('epoch')\n# A hack to force the column headers to wrap.\n#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n# Display the table.\ndf_stats","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:46.178029Z","iopub.execute_input":"2021-09-03T04:00:46.178404Z","iopub.status.idle":"2021-09-03T04:00:46.196809Z","shell.execute_reply.started":"2021-09-03T04:00:46.178365Z","shell.execute_reply":"2021-09-03T04:00:46.195654Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"       Training Loss  Valid. Loss  Training Accur  Valid. Accur.  \\\nepoch                                                              \n1        11136.71418      0.04101         0.97389        0.98186   \n2            0.03675      0.03801         0.98584        0.98467   \n3            0.02933      0.03863         0.98791        0.98388   \n\n      Training Time Validation Time  \nepoch                                \n1           0:25:45         0:01:37  \n2           0:25:29         0:01:37  \n3           0:25:27         0:01:37  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Training Accur</th>\n      <th>Valid. Accur.</th>\n      <th>Training Time</th>\n      <th>Validation Time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>11136.71418</td>\n      <td>0.04101</td>\n      <td>0.97389</td>\n      <td>0.98186</td>\n      <td>0:25:45</td>\n      <td>0:01:37</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.03675</td>\n      <td>0.03801</td>\n      <td>0.98584</td>\n      <td>0.98467</td>\n      <td>0:25:29</td>\n      <td>0:01:37</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.02933</td>\n      <td>0.03863</td>\n      <td>0.98791</td>\n      <td>0.98388</td>\n      <td>0:25:27</td>\n      <td>0:01:37</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Plot the learning curve.\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\n# Label the plot.\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks([1, 2, 3, 4])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:46.198298Z","iopub.execute_input":"2021-09-03T04:00:46.198674Z","iopub.status.idle":"2021-09-03T04:00:46.346843Z","shell.execute_reply.started":"2021-09-03T04:00:46.198637Z","shell.execute_reply":"2021-09-03T04:00:46.345904Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# Plot the accuracy curve.\nplt.plot(df_stats['Training Accur'], 'b-o', label=\"Training\")\nplt.plot(df_stats['Valid. Accur.'], 'g-o', label=\"Validation\")\n\n# Label the plot.\nplt.title(\"Training & Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks([1, 2, 3, 4])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:46.350821Z","iopub.execute_input":"2021-09-03T04:00:46.351082Z","iopub.status.idle":"2021-09-03T04:00:46.489137Z","shell.execute_reply.started":"2021-09-03T04:00:46.351051Z","shell.execute_reply":"2021-09-03T04:00:46.488248Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"## Get Test Metrics Result","metadata":{}},{"cell_type":"code","source":"# get predictions for test data\nthresh = 0.4\nwith torch.no_grad():\n    outputs = model(test_seq[0:1000].to(device), test_mask[0:1000].to(device))\n    pred_probs = torch.sigmoid(outputs.logits)\n    pred_probs = pred_probs.cpu().detach().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:46.491076Z","iopub.execute_input":"2021-09-03T04:00:46.491454Z","iopub.status.idle":"2021-09-03T04:00:49.213513Z","shell.execute_reply.started":"2021-09-03T04:00:46.491418Z","shell.execute_reply":"2021-09-03T04:00:49.212607Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"pred_probs","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:49.214720Z","iopub.execute_input":"2021-09-03T04:00:49.215064Z","iopub.status.idle":"2021-09-03T04:00:49.221956Z","shell.execute_reply.started":"2021-09-03T04:00:49.215027Z","shell.execute_reply":"2021-09-03T04:00:49.221184Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"array([[4.18397546e-01, 6.41216524e-04, 5.67431329e-03, 4.51990432e-04,\n        1.33774608e-01, 7.18725892e-03],\n       [2.83432342e-07, 3.08580650e-09, 3.24021471e-06, 6.16964186e-08,\n        8.41220924e-08, 7.76138709e-09],\n       [5.45564853e-06, 2.58810751e-09, 3.37223639e-07, 1.16589085e-07,\n        6.39215685e-08, 1.60530448e-07],\n       ...,\n       [2.50054745e-05, 1.48842372e-09, 5.17049057e-06, 3.28046177e-08,\n        3.16770837e-07, 4.32330189e-08],\n       [9.98154581e-01, 2.00076863e-01, 9.70898628e-01, 2.14167647e-02,\n        9.07372892e-01, 7.41376817e-01],\n       [3.85794401e-01, 8.03433766e-04, 6.14436030e-01, 7.39134615e-04,\n        3.08352392e-02, 7.70338287e-04]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = (pred_probs > thresh).astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:49.223448Z","iopub.execute_input":"2021-09-03T04:00:49.224138Z","iopub.status.idle":"2021-09-03T04:00:49.230852Z","shell.execute_reply.started":"2021-09-03T04:00:49.224099Z","shell.execute_reply":"2021-09-03T04:00:49.229868Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:49.232249Z","iopub.execute_input":"2021-09-03T04:00:49.232761Z","iopub.status.idle":"2021-09-03T04:00:49.242690Z","shell.execute_reply.started":"2021-09-03T04:00:49.232723Z","shell.execute_reply":"2021-09-03T04:00:49.241898Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"array([[1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0],\n       ...,\n       [0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 1, 1],\n       [0, 0, 1, 0, 0, 0]])"},"metadata":{}}]},{"cell_type":"code","source":"y_true = np.array(test_df[categories])[0:1000]","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:49.245621Z","iopub.execute_input":"2021-09-03T04:00:49.245891Z","iopub.status.idle":"2021-09-03T04:00:49.253269Z","shell.execute_reply.started":"2021-09-03T04:00:49.245857Z","shell.execute_reply":"2021-09-03T04:00:49.252433Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n    '''\n    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n    http://stackoverflow.com/q/32239577/395857\n    \n    Take in np.array for y_true and y_pred. E.g.\n    y_true = np.array([[0,1,0],\n                       [0,1,1],\n                       [1,0,1],\n                       [0,0,1]])\n\n    y_pred = np.array([[0,1,1],\n                       [0,1,1],\n                       [0,1,0],\n                       [0,0,0]])\n    '''\n    acc_list = []\n    for i in range(y_true.shape[0]):\n        set_true = set( np.where(y_true[i])[0] )\n        set_pred = set( np.where(y_pred[i])[0] )\n        #print('\\nset_true: {0}'.format(set_true))\n        #print('set_pred: {0}'.format(set_pred))\n        tmp_a = None\n        if len(set_true) == 0 and len(set_pred) == 0:\n            tmp_a = 1\n        else:\n            tmp_a = len(set_true.intersection(set_pred))/\\\n                    float( len(set_true.union(set_pred)) )\n        #print('tmp_a: {0}'.format(tmp_a))\n        acc_list.append(tmp_a)\n    return np.mean(acc_list)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:49.256009Z","iopub.execute_input":"2021-09-03T04:00:49.256399Z","iopub.status.idle":"2021-09-03T04:00:49.266079Z","shell.execute_reply.started":"2021-09-03T04:00:49.256342Z","shell.execute_reply":"2021-09-03T04:00:49.265223Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(\"accuracy_score:\", accuracy_score(y_true, y_pred))\nprint(\"Hamming_score:\", hamming_score(y_true, y_pred))\nprint(\"Hamming_loss:\", hamming_loss(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:49.267601Z","iopub.execute_input":"2021-09-03T04:00:49.268015Z","iopub.status.idle":"2021-09-03T04:00:49.293392Z","shell.execute_reply.started":"2021-09-03T04:00:49.267982Z","shell.execute_reply":"2021-09-03T04:00:49.292640Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"accuracy_score: 0.929\nHamming_score: 0.9508333333333333\nHamming_loss: 0.013833333333333333\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Saving the Model","metadata":{}},{"cell_type":"code","source":"PATH = \"./toxic_mobileBERT_multilabel\"\ntorch.save(model.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:49.294396Z","iopub.execute_input":"2021-09-03T04:00:49.294755Z","iopub.status.idle":"2021-09-03T04:00:49.542909Z","shell.execute_reply.started":"2021-09-03T04:00:49.294726Z","shell.execute_reply":"2021-09-03T04:00:49.542024Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# 3. Submission","metadata":{}},{"cell_type":"code","source":"len(test_csv)\ntest_csv.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:49.544203Z","iopub.execute_input":"2021-09-03T04:00:49.544565Z","iopub.status.idle":"2021-09-03T04:00:49.555358Z","shell.execute_reply.started":"2021-09-03T04:00:49.544526Z","shell.execute_reply":"2021-09-03T04:00:49.554289Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text\n0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n3  00017563c3f7919a  :If you have a look back at the source, the in...\n4  00017695ad8997eb          I don't anonymously edit articles at all.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>:If you have a look back at the source, the in...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>I don't anonymously edit articles at all.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# tokenize and encode sequences in the actual test set\nsub_tokens = tokenizer.batch_encode_plus(test_csv[\"comment_text\"].tolist(),\n                                         max_length = 200,\n                                         pad_to_max_length=True,\n                                         truncation=True,\n                                         return_token_type_ids=False\n                                         )","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:00:49.557002Z","iopub.execute_input":"2021-09-03T04:00:49.557628Z","iopub.status.idle":"2021-09-03T04:01:32.626017Z","shell.execute_reply.started":"2021-09-03T04:00:49.557591Z","shell.execute_reply":"2021-09-03T04:01:32.625148Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"sub_seq = torch.tensor(sub_tokens['input_ids'])\nsub_mask = torch.tensor(sub_tokens['attention_mask'])","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:01:32.627224Z","iopub.execute_input":"2021-09-03T04:01:32.627571Z","iopub.status.idle":"2021-09-03T04:01:39.230524Z","shell.execute_reply.started":"2021-09-03T04:01:32.627537Z","shell.execute_reply":"2021-09-03T04:01:39.227600Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"sub_data = TensorDataset(sub_seq, sub_mask)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:01:39.234701Z","iopub.execute_input":"2021-09-03T04:01:39.235289Z","iopub.status.idle":"2021-09-03T04:01:39.240405Z","shell.execute_reply.started":"2021-09-03T04:01:39.235241Z","shell.execute_reply":"2021-09-03T04:01:39.239355Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# dataLoader for validation set\nsub_dataloader = DataLoader(sub_data, \n                            batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:01:39.241942Z","iopub.execute_input":"2021-09-03T04:01:39.242464Z","iopub.status.idle":"2021-09-03T04:01:39.249455Z","shell.execute_reply.started":"2021-09-03T04:01:39.242427Z","shell.execute_reply":"2021-09-03T04:01:39.248531Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Measure how long the evaluation going to takes.\nt0 = time.time()\n\nfor step, batch in enumerate(sub_dataloader):\n    # Progress update every 40 batches.\n    if step % 40 == 0 and not step == 0:\n        # Calculate elapsed time in minutes.\n        elapsed = format_time(time.time() - t0)\n        # Report progress.\n        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(sub_dataloader), elapsed))\n    b_input_ids = batch[0].to(device)\n    b_input_mask = batch[1].to(device)\n    with torch.no_grad():\n        outputs = model(b_input_ids, b_input_mask)\n        pred_probs = torch.sigmoid(outputs.logits)\n        if step == 0:\n            predictions = pred_probs.cpu().detach().numpy()\n        else:\n            predictions = np.append(predictions, pred_probs.cpu().detach().numpy(), axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:01:39.250841Z","iopub.execute_input":"2021-09-03T04:01:39.251278Z","iopub.status.idle":"2021-09-03T04:09:24.623706Z","shell.execute_reply.started":"2021-09-03T04:01:39.251240Z","shell.execute_reply":"2021-09-03T04:09:24.622660Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"  Batch    40  of  4,787.    Elapsed: 0:00:04.\n  Batch    80  of  4,787.    Elapsed: 0:00:08.\n  Batch   120  of  4,787.    Elapsed: 0:00:12.\n  Batch   160  of  4,787.    Elapsed: 0:00:16.\n  Batch   200  of  4,787.    Elapsed: 0:00:19.\n  Batch   240  of  4,787.    Elapsed: 0:00:23.\n  Batch   280  of  4,787.    Elapsed: 0:00:27.\n  Batch   320  of  4,787.    Elapsed: 0:00:31.\n  Batch   360  of  4,787.    Elapsed: 0:00:35.\n  Batch   400  of  4,787.    Elapsed: 0:00:39.\n  Batch   440  of  4,787.    Elapsed: 0:00:43.\n  Batch   480  of  4,787.    Elapsed: 0:00:47.\n  Batch   520  of  4,787.    Elapsed: 0:00:50.\n  Batch   560  of  4,787.    Elapsed: 0:00:54.\n  Batch   600  of  4,787.    Elapsed: 0:00:58.\n  Batch   640  of  4,787.    Elapsed: 0:01:02.\n  Batch   680  of  4,787.    Elapsed: 0:01:06.\n  Batch   720  of  4,787.    Elapsed: 0:01:10.\n  Batch   760  of  4,787.    Elapsed: 0:01:14.\n  Batch   800  of  4,787.    Elapsed: 0:01:18.\n  Batch   840  of  4,787.    Elapsed: 0:01:22.\n  Batch   880  of  4,787.    Elapsed: 0:01:25.\n  Batch   920  of  4,787.    Elapsed: 0:01:29.\n  Batch   960  of  4,787.    Elapsed: 0:01:33.\n  Batch 1,000  of  4,787.    Elapsed: 0:01:37.\n  Batch 1,040  of  4,787.    Elapsed: 0:01:41.\n  Batch 1,080  of  4,787.    Elapsed: 0:01:45.\n  Batch 1,120  of  4,787.    Elapsed: 0:01:49.\n  Batch 1,160  of  4,787.    Elapsed: 0:01:53.\n  Batch 1,200  of  4,787.    Elapsed: 0:01:56.\n  Batch 1,240  of  4,787.    Elapsed: 0:02:00.\n  Batch 1,280  of  4,787.    Elapsed: 0:02:04.\n  Batch 1,320  of  4,787.    Elapsed: 0:02:08.\n  Batch 1,360  of  4,787.    Elapsed: 0:02:12.\n  Batch 1,400  of  4,787.    Elapsed: 0:02:16.\n  Batch 1,440  of  4,787.    Elapsed: 0:02:20.\n  Batch 1,480  of  4,787.    Elapsed: 0:02:24.\n  Batch 1,520  of  4,787.    Elapsed: 0:02:28.\n  Batch 1,560  of  4,787.    Elapsed: 0:02:31.\n  Batch 1,600  of  4,787.    Elapsed: 0:02:35.\n  Batch 1,640  of  4,787.    Elapsed: 0:02:39.\n  Batch 1,680  of  4,787.    Elapsed: 0:02:43.\n  Batch 1,720  of  4,787.    Elapsed: 0:02:47.\n  Batch 1,760  of  4,787.    Elapsed: 0:02:51.\n  Batch 1,800  of  4,787.    Elapsed: 0:02:55.\n  Batch 1,840  of  4,787.    Elapsed: 0:02:59.\n  Batch 1,880  of  4,787.    Elapsed: 0:03:02.\n  Batch 1,920  of  4,787.    Elapsed: 0:03:06.\n  Batch 1,960  of  4,787.    Elapsed: 0:03:10.\n  Batch 2,000  of  4,787.    Elapsed: 0:03:14.\n  Batch 2,040  of  4,787.    Elapsed: 0:03:18.\n  Batch 2,080  of  4,787.    Elapsed: 0:03:22.\n  Batch 2,120  of  4,787.    Elapsed: 0:03:26.\n  Batch 2,160  of  4,787.    Elapsed: 0:03:30.\n  Batch 2,200  of  4,787.    Elapsed: 0:03:34.\n  Batch 2,240  of  4,787.    Elapsed: 0:03:37.\n  Batch 2,280  of  4,787.    Elapsed: 0:03:41.\n  Batch 2,320  of  4,787.    Elapsed: 0:03:45.\n  Batch 2,360  of  4,787.    Elapsed: 0:03:49.\n  Batch 2,400  of  4,787.    Elapsed: 0:03:53.\n  Batch 2,440  of  4,787.    Elapsed: 0:03:57.\n  Batch 2,480  of  4,787.    Elapsed: 0:04:01.\n  Batch 2,520  of  4,787.    Elapsed: 0:04:05.\n  Batch 2,560  of  4,787.    Elapsed: 0:04:09.\n  Batch 2,600  of  4,787.    Elapsed: 0:04:12.\n  Batch 2,640  of  4,787.    Elapsed: 0:04:16.\n  Batch 2,680  of  4,787.    Elapsed: 0:04:20.\n  Batch 2,720  of  4,787.    Elapsed: 0:04:24.\n  Batch 2,760  of  4,787.    Elapsed: 0:04:28.\n  Batch 2,800  of  4,787.    Elapsed: 0:04:32.\n  Batch 2,840  of  4,787.    Elapsed: 0:04:36.\n  Batch 2,880  of  4,787.    Elapsed: 0:04:40.\n  Batch 2,920  of  4,787.    Elapsed: 0:04:44.\n  Batch 2,960  of  4,787.    Elapsed: 0:04:47.\n  Batch 3,000  of  4,787.    Elapsed: 0:04:51.\n  Batch 3,040  of  4,787.    Elapsed: 0:04:55.\n  Batch 3,080  of  4,787.    Elapsed: 0:04:59.\n  Batch 3,120  of  4,787.    Elapsed: 0:05:03.\n  Batch 3,160  of  4,787.    Elapsed: 0:05:07.\n  Batch 3,200  of  4,787.    Elapsed: 0:05:11.\n  Batch 3,240  of  4,787.    Elapsed: 0:05:15.\n  Batch 3,280  of  4,787.    Elapsed: 0:05:19.\n  Batch 3,320  of  4,787.    Elapsed: 0:05:23.\n  Batch 3,360  of  4,787.    Elapsed: 0:05:26.\n  Batch 3,400  of  4,787.    Elapsed: 0:05:30.\n  Batch 3,440  of  4,787.    Elapsed: 0:05:34.\n  Batch 3,480  of  4,787.    Elapsed: 0:05:38.\n  Batch 3,520  of  4,787.    Elapsed: 0:05:42.\n  Batch 3,560  of  4,787.    Elapsed: 0:05:46.\n  Batch 3,600  of  4,787.    Elapsed: 0:05:50.\n  Batch 3,640  of  4,787.    Elapsed: 0:05:54.\n  Batch 3,680  of  4,787.    Elapsed: 0:05:58.\n  Batch 3,720  of  4,787.    Elapsed: 0:06:01.\n  Batch 3,760  of  4,787.    Elapsed: 0:06:05.\n  Batch 3,800  of  4,787.    Elapsed: 0:06:09.\n  Batch 3,840  of  4,787.    Elapsed: 0:06:13.\n  Batch 3,880  of  4,787.    Elapsed: 0:06:17.\n  Batch 3,920  of  4,787.    Elapsed: 0:06:21.\n  Batch 3,960  of  4,787.    Elapsed: 0:06:25.\n  Batch 4,000  of  4,787.    Elapsed: 0:06:29.\n  Batch 4,040  of  4,787.    Elapsed: 0:06:33.\n  Batch 4,080  of  4,787.    Elapsed: 0:06:37.\n  Batch 4,120  of  4,787.    Elapsed: 0:06:40.\n  Batch 4,160  of  4,787.    Elapsed: 0:06:44.\n  Batch 4,200  of  4,787.    Elapsed: 0:06:48.\n  Batch 4,240  of  4,787.    Elapsed: 0:06:52.\n  Batch 4,280  of  4,787.    Elapsed: 0:06:56.\n  Batch 4,320  of  4,787.    Elapsed: 0:07:00.\n  Batch 4,360  of  4,787.    Elapsed: 0:07:04.\n  Batch 4,400  of  4,787.    Elapsed: 0:07:08.\n  Batch 4,440  of  4,787.    Elapsed: 0:07:12.\n  Batch 4,480  of  4,787.    Elapsed: 0:07:15.\n  Batch 4,520  of  4,787.    Elapsed: 0:07:19.\n  Batch 4,560  of  4,787.    Elapsed: 0:07:23.\n  Batch 4,600  of  4,787.    Elapsed: 0:07:27.\n  Batch 4,640  of  4,787.    Elapsed: 0:07:31.\n  Batch 4,680  of  4,787.    Elapsed: 0:07:35.\n  Batch 4,720  of  4,787.    Elapsed: 0:07:39.\n  Batch 4,760  of  4,787.    Elapsed: 0:07:43.\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions_df = pd.DataFrame(predictions, columns = categories)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:09:24.627853Z","iopub.execute_input":"2021-09-03T04:09:24.628221Z","iopub.status.idle":"2021-09-03T04:09:24.636083Z","shell.execute_reply.started":"2021-09-03T04:09:24.628185Z","shell.execute_reply":"2021-09-03T04:09:24.634992Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"len(predictions_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:09:24.640669Z","iopub.execute_input":"2021-09-03T04:09:24.641055Z","iopub.status.idle":"2021-09-03T04:09:24.652087Z","shell.execute_reply.started":"2021-09-03T04:09:24.641022Z","shell.execute_reply":"2021-09-03T04:09:24.651003Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"153164"},"metadata":{}}]},{"cell_type":"code","source":"submission = pd.concat([test_csv[\"id\"], predictions_df], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:09:24.656133Z","iopub.execute_input":"2021-09-03T04:09:24.656477Z","iopub.status.idle":"2021-09-03T04:09:24.672485Z","shell.execute_reply.started":"2021-09-03T04:09:24.656445Z","shell.execute_reply":"2021-09-03T04:09:24.671746Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:09:24.675912Z","iopub.execute_input":"2021-09-03T04:09:24.677861Z","iopub.status.idle":"2021-09-03T04:09:24.700964Z","shell.execute_reply.started":"2021-09-03T04:09:24.677825Z","shell.execute_reply":"2021-09-03T04:09:24.700253Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"                 id        toxic  severe_toxic      obscene       threat  \\\n0  00001cee341fdb12  9.92127e-01   2.12684e-01  9.72361e-01  6.37905e-02   \n1  0000247867823ef7  2.44336e-06   5.30143e-09  2.94156e-05  2.21455e-08   \n2  00013b17ad220c46  1.62982e-05   2.38196e-08  2.83707e-05  8.08777e-08   \n3  00017563c3f7919a  4.30704e-08   6.36729e-10  4.56163e-07  7.97023e-08   \n4  00017695ad8997eb  2.98555e-05   8.15860e-09  7.76681e-06  1.26799e-07   \n\n        insult  identity_hate  \n0  9.12360e-01    4.10992e-01  \n1  9.99970e-07    1.26706e-07  \n2  5.54158e-07    1.63007e-07  \n3  3.92876e-09    1.99919e-09  \n4  6.22114e-07    2.37654e-07  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>9.92127e-01</td>\n      <td>2.12684e-01</td>\n      <td>9.72361e-01</td>\n      <td>6.37905e-02</td>\n      <td>9.12360e-01</td>\n      <td>4.10992e-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>2.44336e-06</td>\n      <td>5.30143e-09</td>\n      <td>2.94156e-05</td>\n      <td>2.21455e-08</td>\n      <td>9.99970e-07</td>\n      <td>1.26706e-07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>1.62982e-05</td>\n      <td>2.38196e-08</td>\n      <td>2.83707e-05</td>\n      <td>8.08777e-08</td>\n      <td>5.54158e-07</td>\n      <td>1.63007e-07</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>4.30704e-08</td>\n      <td>6.36729e-10</td>\n      <td>4.56163e-07</td>\n      <td>7.97023e-08</td>\n      <td>3.92876e-09</td>\n      <td>1.99919e-09</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>2.98555e-05</td>\n      <td>8.15860e-09</td>\n      <td>7.76681e-06</td>\n      <td>1.26799e-07</td>\n      <td>6.22114e-07</td>\n      <td>2.37654e-07</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:09:24.704635Z","iopub.execute_input":"2021-09-03T04:09:24.706789Z","iopub.status.idle":"2021-09-03T04:09:26.328140Z","shell.execute_reply.started":"2021-09-03T04:09:24.706752Z","shell.execute_reply":"2021-09-03T04:09:26.327295Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"# 4. Test for runtime for CPU","metadata":{}},{"cell_type":"code","source":"# instantiate model using \"google/mobilebert-uncased\"\ncheckpoint = \"google/mobilebert-uncased\"\nPATH = \"./toxic_mobileBERT_multilabel\"\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = 6)\nmodel.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:09:26.329537Z","iopub.execute_input":"2021-09-03T04:09:26.329887Z","iopub.status.idle":"2021-09-03T04:09:28.658786Z","shell.execute_reply.started":"2021-09-03T04:09:26.329851Z","shell.execute_reply":"2021-09-03T04:09:28.657778Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:09:28.660279Z","iopub.execute_input":"2021-09-03T04:09:28.660723Z","iopub.status.idle":"2021-09-03T04:09:28.709491Z","shell.execute_reply.started":"2021-09-03T04:09:28.660682Z","shell.execute_reply":"2021-09-03T04:09:28.708529Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"MobileBertForSequenceClassification(\n  (mobilebert): MobileBertModel(\n    (embeddings): MobileBertEmbeddings(\n      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 512)\n      (token_type_embeddings): Embedding(2, 512)\n      (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n      (LayerNorm): NoNorm()\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): MobileBertEncoder(\n      (layer): ModuleList(\n        (0): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (1): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (2): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (3): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (4): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (5): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (6): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (7): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (8): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (9): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (10): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (11): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (12): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (13): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (14): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (15): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (16): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (17): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (18): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (19): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (20): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (21): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (22): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n        (23): MobileBertLayer(\n          (attention): MobileBertAttention(\n            (self): MobileBertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=512, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): MobileBertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (intermediate): MobileBertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n          )\n          (output): MobileBertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): NoNorm()\n            (bottleneck): OutputBottleneck(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (LayerNorm): NoNorm()\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (bottleneck): Bottleneck(\n            (input): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n            (attention): BottleneckLayer(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): NoNorm()\n            )\n          )\n          (ffn): ModuleList(\n            (0): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (1): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n            (2): FFNLayer(\n              (intermediate): MobileBertIntermediate(\n                (dense): Linear(in_features=128, out_features=512, bias=True)\n              )\n              (output): FFNOutput(\n                (dense): Linear(in_features=512, out_features=128, bias=True)\n                (LayerNorm): NoNorm()\n              )\n            )\n          )\n        )\n      )\n    )\n    (pooler): MobileBertPooler()\n  )\n  (dropout): Dropout(p=0.0, inplace=False)\n  (classifier): Linear(in_features=512, out_features=6, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Measure how long the evaluation going to takes.\nt0 = time.time()\n\nfor step, batch in enumerate(sub_dataloader):\n    # Progress update every 40 batches.\n    if step % 40 == 0 and not step == 0:\n        # Calculate elapsed time in minutes.\n        elapsed = format_time(time.time() - t0)\n        # Report progress.\n        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(sub_dataloader), elapsed))\n    b_input_ids = batch[0].to(device)\n    b_input_mask = batch[1].to(device)\n    with torch.no_grad():\n        outputs = model(b_input_ids, b_input_mask)\n        pred_probs = torch.sigmoid(outputs.logits)\n        if step == 0:\n            predictions = pred_probs.cpu().detach().numpy()\n        else:\n            predictions = np.append(predictions, pred_probs.cpu().detach().numpy(), axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T04:09:28.710701Z","iopub.execute_input":"2021-09-03T04:09:28.711038Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"  Batch    40  of  4,787.    Elapsed: 0:02:26.\n  Batch    80  of  4,787.    Elapsed: 0:04:53.\n  Batch   120  of  4,787.    Elapsed: 0:07:20.\n  Batch   160  of  4,787.    Elapsed: 0:09:46.\n  Batch   200  of  4,787.    Elapsed: 0:12:12.\n  Batch   240  of  4,787.    Elapsed: 0:14:37.\n  Batch   280  of  4,787.    Elapsed: 0:17:03.\n  Batch   320  of  4,787.    Elapsed: 0:19:29.\n","output_type":"stream"}]}]}